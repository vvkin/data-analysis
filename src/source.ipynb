{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arabic-language",
   "metadata": {},
   "source": [
    "# Laboratory work N1\n",
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspected-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, Any\n",
    "import configparser\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor, RealDictCursor\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hawaiian-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elect-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(filename :str = 'database.ini', section: str ='postgresql') -> dict[str, Any]:\n",
    "    parser = configparser.ConfigParser()\n",
    "    parser.read(filename)\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        return dict(params)\n",
    "    else: raise Exception('Invalid .ini file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-greenhouse",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "logical-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def camel_to_snake(string: str) -> str:\n",
    "    string = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', string)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', string).lower()\n",
    "\n",
    "def remove_consecutive_underscores(string: str) -> str:\n",
    "    return re.sub('_+', '_', string)\n",
    "        \n",
    "def get_rename_mapping(columns: list[str]) -> dict[str, str]:\n",
    "    renamed_columns = map(remove_consecutive_underscores,\n",
    "                          map(camel_to_snake, columns))\n",
    "    mapping = zip(columns, renamed_columns)\n",
    "    return dict(mapping)\n",
    "\n",
    "def extract_number(string: str) -> str:\n",
    "    numbers = re.findall('\\d+', str(string))\n",
    "    return numbers[0] if numbers else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "circular-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(f_series: pd.Series, s_series: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.intersect1d(f_series.dropna(), s_series.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impossible-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(data: pd.Series) -> pd.DataFrame:\n",
    "    dates = pd.to_datetime(data, errors='coerce')\n",
    "    df = pd.DataFrame({\n",
    "        'day': dates.dt.day,\n",
    "        'month': dates.dt.month,\n",
    "        'year': dates.dt.year,\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funny-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_string(values_number: int) -> str:\n",
    "    return '(' + ('%s,' * values_number)[:-1] + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-standard",
   "metadata": {},
   "source": [
    "## Postgres utils\n",
    "### QuerySet wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "miniature-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuerySet:\n",
    "    def __init__(self, query_set):\n",
    "        self.query_set = query_set\n",
    "\n",
    "    def all(self):\n",
    "        if self.query_set:\n",
    "            return self.query_set.fetchall()\n",
    "        else: return None\n",
    "\n",
    "    def one(self):\n",
    "        if self.query_set:\n",
    "            return self.query_set.fetchone()\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-istanbul",
   "metadata": {},
   "source": [
    "### Postgres API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dangerous-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PgAPI:\n",
    "    @staticmethod\n",
    "    def get_db():\n",
    "        params = config()\n",
    "        conn = psycopg2.connect(**params)\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_cursor(cursor_factory=None):\n",
    "        return PgAPI.get_db().cursor(cursor_factory=cursor_factory)\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_query(query: str, *args) -> QuerySet:\n",
    "        cursor = PgAPI.get_cursor()\n",
    "        cursor.execute(query, args)\n",
    "        return QuerySet(cursor)\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_dict_query(query: str, *args) -> QuerySet:\n",
    "        cursor = PgAPI.get_cursor(DictCursor)\n",
    "        cursor.execute(query, args)\n",
    "        return QuerySet(cursor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_rdict_query(query: str, *args) -> QuerySet:\n",
    "        cursor = PgAPI.get_cursor(RealDictCursor)\n",
    "        cursor.execute(query, args)\n",
    "        return QuerySet(cursor)\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_call(query: str, *args) -> None:\n",
    "        cursor = PgAPI.get_cursor()\n",
    "        cursor.execute(query, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-inquiry",
   "metadata": {},
   "source": [
    "### Database utils (init / drop / clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "communist-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db(file_name: str = 'schema.sql') -> None:\n",
    "    with open(file_name, 'r') as fhand:\n",
    "        query = fhand.read()\n",
    "        PgAPI.execute_call(query)\n",
    "\n",
    "def init_dates() -> None:\n",
    "    query = 'CALL fill_dates()'\n",
    "    PgAPI.execute_call(query)\n",
    "\n",
    "def clear_db() -> None:\n",
    "    t_query = \"SELECT tablename FROM pg_tables\\\n",
    "        WHERE schemaname='public'\"\n",
    "    query_set = PgAPI.execute_query(t_query).all()\n",
    "    for record in query_set:\n",
    "        query = 'TRUNCATE ' + record[0] + ' CASCADE'\n",
    "        PgAPI.execute_call(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-combine",
   "metadata": {},
   "source": [
    "### Create SQLAlchemy engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config()\n",
    "params['username'] = params.pop('user')  # rename key\n",
    "params['drivername'] = 'postgresql'\n",
    "conn_url = URL(**params)\n",
    "engine = create_engine(conn_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-spirituality",
   "metadata": {},
   "source": [
    "### QueryBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recovered-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(t_name: str, columns: list[str]) -> str:\n",
    "    q_columns = ', '.join(columns)\n",
    "    q_from = ' '.join(['FROM', t_name])\n",
    "    q_where = 'WHERE day IS NULL AND month IS NULL' \\\n",
    "        if t_name == 'dates' and len(columns) < 3 else ''\n",
    "    return ' '.join(['SELECT', q_columns, q_from, q_where])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "traditional-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(t_name: str, df: pd.DataFrame) -> None:\n",
    "    df.to_sql(t_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "czech-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where(columns: list[str], operators: list[str], values: list[str]) -> str:\n",
    "    clause_body = ' AND '.join([\n",
    "        ' '.join([columns[i], operators[i], values[i]])\n",
    "        for i in range(len(columns))\n",
    "    ])\n",
    "    return ' '.join(['WHERE', clause_body])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-reception",
   "metadata": {},
   "source": [
    "## Providers beetwen pandas dataframes and database tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-mattress",
   "metadata": {},
   "source": [
    "### Insert unique data from dataset to the database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "finished-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_unique(t_name: str, df: pd.DataFrame, fields: Union[str, list[str], dict[str, str]]) -> None:\n",
    "    if isinstance(fields, str): fields = [fields]\n",
    "    \n",
    "    if isinstance(fields, dict):\n",
    "        df_cols, db_cols = zip(*fields.items())\n",
    "    else: df_cols = db_cols = tuple(fields)\n",
    "    \n",
    "    query = select(t_name, db_cols)\n",
    "    data = PgAPI.execute_dict_query(query).all()\n",
    "    df_data = df[list(df_cols)].drop_duplicates()\n",
    "    mapping = dict(zip(df_cols, db_cols))\n",
    "    \n",
    "    if not data:\n",
    "        insert(t_name, df_data.rename(columns=mapping))\n",
    "        return\n",
    "    \n",
    "    db_data = pd.DataFrame(data, columns=db_cols)\n",
    "    db_data.loc[-1] = df_data.loc[0]\n",
    "    \n",
    "    new_data = df_data.merge(\n",
    "        right=db_data, how='left',\n",
    "        left_on=df_cols, right_on=db_cols,\n",
    "        indicator=True\n",
    "    )\n",
    "    drops = ['_merge', *set(db_cols).difference(df_cols)]\n",
    "    \n",
    "    to_insert = new_data[new_data['_merge']=='left_only']\\\n",
    "        .drop(columns=drops)\\\n",
    "        .rename(columns=mapping)\n",
    "\n",
    "    insert(t_name, to_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-handy",
   "metadata": {},
   "source": [
    "### Join dataframe and database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "common-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(t_name: str, df: pd.DataFrame, fields: Union[str, list[str], dict[str, str]], out: str) -> pd.DataFrame:\n",
    "    if isinstance(fields, str): fields = [fields]\n",
    "        \n",
    "    if isinstance(fields, dict):\n",
    "        df_cols, db_cols = zip(*fields.items())\n",
    "    else: df_cols = db_cols = tuple(fields)\n",
    "    \n",
    "    query = select(t_name,  ('id', ) + db_cols)\n",
    "    data = PgAPI.execute_dict_query(query).all()\n",
    "    db_data = pd.DataFrame(data, columns=('id', ) + db_cols)\n",
    "    df_data = df[list(df_cols)]\n",
    "    \n",
    "    join_ids = df_data.merge(\n",
    "        right=db_data, how='left',\n",
    "        left_on=df_cols, right_on=db_cols\n",
    "    )['id']\n",
    "    \n",
    "    to_return = df.drop(list(df_cols), axis=1)\n",
    "    to_return[out] = join_ids\n",
    "    to_return = to_return.reset_index(drop=True)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-string",
   "metadata": {},
   "source": [
    "## Mapping between dataframes and tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-roommate",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incident-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, df: pd.DataFrame, fields: list[str], \n",
    "            fact_type: str) -> None:\n",
    "        self.df = df[fields]\n",
    "        self.fact_type = fact_type\n",
    "        self.config_name = ''.join([fact_type, '.json'])\n",
    "       \n",
    "    def rename(self, mapping: dict[str, str]) -> None:\n",
    "        self.df = self.df.rename(columns=mapping)\n",
    "    \n",
    "    def dropna(self, columns: list[str]) -> None:\n",
    "        self.df = self.df.dropna(subset=columns)\n",
    "            \n",
    "    def load_foreign(self, mapping: dict[str, dict[str, str]]) -> None:            \n",
    "        for t_name in mapping:\n",
    "            \n",
    "            params = mapping[t_name]\n",
    "            if not isinstance(params[1], list):\n",
    "                params = [params]\n",
    "\n",
    "            for param in params:\n",
    "                fields, id_name = param\n",
    "                insert_unique(t_name, self.df, fields)\n",
    "                self.df = join(t_name, self.df, fields, id_name)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return repr(self.df)\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        with open(f'configs/{self.config_name}') as fhand:\n",
    "            data = json.load(fhand)\n",
    "            self.load_foreign(data)\n",
    "    \n",
    "    def load_fact(self) -> None:\n",
    "        fact = Fact(self.df, self.fact_type)\n",
    "        insert('facts', fact.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dynamic-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact(Table):\n",
    "    def __init__(self, df: pd.DataFrame, f_type: str) -> None:\n",
    "        fields = list(df.columns)\n",
    "        super().__init__(df, fields, '')\n",
    "        self.df['type_name'] = f_type\n",
    "        self.load_foreign({\n",
    "            'fact_types': [['type_name'], 'type_id']\n",
    "        })\n",
    "        self.dropna(['movie_id', 'movie_date_id', 'type_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regular-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranking(Table):\n",
    "    def __init__(self, df: pd.DataFrame, year: int) -> None:\n",
    "        fields = [el for el in df.columns if el not in [\n",
    "            'Fandango_Stars', 'Fandango_Difference',\n",
    "            'RT_norm_round', 'RT_user_norm_round',\n",
    "            'Metacritic_user_norm_round',\n",
    "            'IMDB_norm_round', 'Metacritic_norm_round'\n",
    "        ]]\n",
    "        super().__init__(df, fields, 'ranking')\n",
    "        mapping = get_rename_mapping(self.df.columns)\n",
    "        self.rename(mapping)\n",
    "        self._parse_dates()\n",
    "        self._parse_films()\n",
    "        self._init_ids()\n",
    "        self.dropna(['film', 'year'])\n",
    "        self.df['ranking_year'] = year\n",
    "    \n",
    "    def _parse_dates(self) -> None:\n",
    "        years = self.df['film'].apply(\n",
    "            lambda s: s[s.index('(')+1:-1]\n",
    "        ).astype(int)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.df['year'] = years\n",
    "    \n",
    "    def _parse_films(self) -> None:\n",
    "        self.df['film'] = self.df['film'].apply(\n",
    "            lambda s: s[:s.index('(')]\n",
    "        )\n",
    "    \n",
    "    def _init_ids(self) -> None:\n",
    "        types = ['fandango', 'metacritic', 'rotten_tomatoes', 'imdb']\n",
    "        query = 'SELECT id FROM ranking_types WHERE type_name = %s'\n",
    "        for type_name in types:\n",
    "            id_name = '_'.join([type_name, 'type', 'id'])\n",
    "            query_set = PgAPI.execute_query(query, type_name)\n",
    "            self.df[id_name] = query_set.one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "concrete-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movie(Table):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        fields = [el for el in df.columns if el not in [\n",
    "            'title', 'imdb_title_id',\n",
    "            'metascore', 'year', 'actors'\n",
    "        ]]\n",
    "        super().__init__(df, fields, 'movie')\n",
    "        self.rename({\n",
    "            'original_title': 'name',\n",
    "            'reviews_from_critics': 'critics_reviews',\n",
    "            'reviews_from_users': 'users_reviews',\n",
    "            'usa_gross_income': 'usa_income',\n",
    "            'worlwide_gross_income': 'worldwide_income',\n",
    "            'votes': 'users_votes',\n",
    "            'avg_vote': 'users_score'\n",
    "        })\n",
    "        self._parse_dates()\n",
    "        self._parse_currencies()\n",
    "        self.dropna([\n",
    "            'director', 'language', 'production_company',\n",
    "            'writer', 'genre', 'year', 'country', 'name'\n",
    "        ])\n",
    "        self.df['type_name'] = 'imdb'\n",
    "    \n",
    "    def _parse_currencies(self) -> None:\n",
    "        for column in ['usa_income', 'worldwide_income', 'budget']:\n",
    "            self.df[column] = self.df[column]\\\n",
    "                .apply(extract_number)\\\n",
    "                .astype(float)\n",
    "        \n",
    "    def _parse_dates(self) -> None:\n",
    "        date_parts = extract_date(self.df['date_published'])\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.df = pd.concat([self.df, date_parts], axis=1)\n",
    "        self.df = self.df.drop(['date_published'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hollow-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oscar(Table):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        fields = list(df.columns)\n",
    "        super().__init__(df, fields, 'oscar')\n",
    "        self.rename({\n",
    "            'ceremony': 'ceremony_number',\n",
    "            'winner': 'is_winner'\n",
    "        })\n",
    "        self.dropna(['film', 'category', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-elite",
   "metadata": {},
   "source": [
    "## Load datasets from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "posted-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('datasets/imdb.csv')\n",
    "oscars = pd.read_csv('datasets/oscars.csv')\n",
    "rankings = pd.read_csv('datasets/ranking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-soccer",
   "metadata": {},
   "source": [
    "## Fill database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "former-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_db()\n",
    "init_db()\n",
    "init_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-keyboard",
   "metadata": {},
   "source": [
    "## Fill types tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-bobby",
   "metadata": {},
   "source": [
    "### Fill fact types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "revolutionary-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_types = ('oscar', 'movie', 'ranking')\n",
    "values_string = ('(%s),' * len(fact_types))[:-1]\n",
    "PgAPI.execute_call('INSERT INTO fact_types (type_name) VALUES ' + values_string, *fact_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-quest",
   "metadata": {},
   "source": [
    "### Fill ranking types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "virgin-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_types = ('imdb', 'metacritic', 'rotten_tomatoes', 'fandango')\n",
    "values_string = ('(%s),' * len(ranking_types))[:-1]\n",
    "PgAPI.execute_call('INSERT INTO ranking_types (type_name) VALUES ' + values_string, *ranking_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-celebrity",
   "metadata": {},
   "source": [
    "## Load datasets to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "everyday-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = Ranking(rankings, year=2015)\n",
    "ranking.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "palestinian-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = Movie(movies)\n",
    "movie.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "restricted-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar = Oscar(oscars)\n",
    "oscar.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-curtis",
   "metadata": {},
   "source": [
    "## Load facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "special-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.load_fact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "invisible-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar.load_fact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "traditional-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking.load_fact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
